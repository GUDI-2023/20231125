{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8599f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import warnings\n",
    "from models.metric import *\n",
    "from models.utils import load_data\n",
    "import time\n",
    "from models.adapt.networks import Net\n",
    "from models.adapt import util\n",
    "from torch_geometric.data import DataLoader\n",
    "from models.adapt.inj_cora_dataset import InjCoraDataset\n",
    "from models.adapt.OTC_dataset import BitcoinOTC\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import (from_networkx)\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    Dataset,\n",
    "    download_url,\n",
    "    extract_gz,\n",
    ")\n",
    "def minmaxscaler(data):\n",
    "    min = torch.min(data)\n",
    "    max = torch.max(data)\n",
    "    return (data - min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32eb8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8631\tRecall: 0.5217\n"
     ]
    }
   ],
   "source": [
    "# Cora dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('str_pretrain.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('feat_pretrain.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_cora')\n",
    "k_all = sum(data.y)\n",
    "# load sampled subgraph with 30 nodes\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_cora/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 1433\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:0'\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_cora.pth'))\n",
    "classifier_model.eval()\n",
    "\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    \n",
    "    s_recover = model1.forward_di(data_pre)\n",
    "    f_recover = model2.forward_di(data_pre)\n",
    "    \n",
    "    for t in range(0,3):      \n",
    "        out, prop = classifier_model(model1.toden(s_recover,f_recover))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        prop_s = prop[:, 0]\n",
    "        prop_f = prop[:, 1]\n",
    "        correct += pred.eq(data_pre.y).sum().item()\n",
    "        loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "        s_recover = model1.conditional_di(s_recover,prop_s)\n",
    "        f_recover = model2.conditional_di(f_recover,prop_f)\n",
    "\n",
    "    ss = model1.loss_func(data_pre , s_recover)\n",
    "    sf = model2.loss_func(data_pre.x, f_recover)\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "\n",
    "y_label = np.array(y_label_list)\n",
    "pred_t = np.array(pred_list)\n",
    "data.y = data.y.bool().int()\n",
    "fs = torch.zeros_like(ss)\n",
    "# 0-1 scale\n",
    "ss = minmaxscaler(F.normalize(ss, p=2, dim=-1))\n",
    "sf = minmaxscaler(F.normalize(sf, p=2, dim=-1))\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    fs[i] += args.lamba * pred_t[i] *ss[i] + (1-args.lamba)*(1-pred_t[i]) * sf[i]\n",
    "auc.append(eval_roc_auc(data.y, fs))\n",
    "rec.append(eval_recall_at_k(data.y, fs, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e9d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9231\tRecall: 0.8012\n"
     ]
    }
   ],
   "source": [
    "# Amazon dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('str_pretrain.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('feat_pretrain.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_amazon')\n",
    "k_all = sum(data.y)\n",
    "# load sampled subgraph with 30 nodes\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_amazon/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 767\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:1' # distributed training and inference\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_ama.pth'))\n",
    "classifier_model.eval()\n",
    "\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    \n",
    "    s_recover = model1.forward_di(data_pre)\n",
    "    f_recover = model2.forward_di(data_pre)\n",
    "    \n",
    "    for t in range(0,3):      \n",
    "        out, prop = classifier_model(model1.toden(s_recover,f_recover))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        prop_s = prop[:, 0]\n",
    "        prop_f = prop[:, 1]\n",
    "        correct += pred.eq(data_pre.y).sum().item()\n",
    "        loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "        s_recover = model1.conditional_di(s_recover,prop_s)\n",
    "        f_recover = model2.conditional_di(f_recover,prop_f)\n",
    "\n",
    "    ss = model1.loss_func(data_pre , s_recover)\n",
    "    sf = model2.loss_func(data_pre.x, f_recover)\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "\n",
    "y_label = np.array(y_label_list)\n",
    "pred_t = np.array(pred_list)\n",
    "data.y = data.y.bool().int()\n",
    "fs = torch.zeros_like(ss)\n",
    "# 0-1 scale\n",
    "ss = minmaxscaler(F.normalize(ss, p=2, dim=-1))\n",
    "sf = minmaxscaler(F.normalize(sf, p=2, dim=-1))\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    fs[i] += args.lamba * pred_t[i] *ss[i] + (1-args.lamba)*(1-pred_t[i]) * sf[i]\n",
    "auc.append(eval_roc_auc(data.y, fs))\n",
    "rec.append(eval_recall_at_k(data.y, fs, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb170a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9349\tRecall: 0.5931\n"
     ]
    }
   ],
   "source": [
    "# Flickr dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('str_pretrain.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('feat_pretrain.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_flickr')\n",
    "k_all = sum(data.y)\n",
    "# load sampled subgraph with 30 nodes\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_flickr/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 500\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:2' # distributed training and inference\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_fli.pth'))\n",
    "classifier_model.eval()\n",
    "\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    \n",
    "    s_recover = model1.forward_di(data_pre)\n",
    "    f_recover = model2.forward_di(data_pre)\n",
    "    \n",
    "    for t in range(0,3):      \n",
    "        out, prop = classifier_model(model1.toden(s_recover,f_recover))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        prop_s = prop[:, 0]\n",
    "        prop_f = prop[:, 1]\n",
    "        correct += pred.eq(data_pre.y).sum().item()\n",
    "        loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "        s_recover = model1.conditional_di(s_recover,prop_s)\n",
    "        f_recover = model2.conditional_di(f_recover,prop_f)\n",
    "\n",
    "    ss = model1.loss_func(data_pre , s_recover)\n",
    "    sf = model2.loss_func(data_pre.x, f_recover)\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "\n",
    "y_label = np.array(y_label_list)\n",
    "pred_t = np.array(pred_list)\n",
    "data.y = data.y.bool().int()\n",
    "fs = torch.zeros_like(ss)\n",
    "# 0-1 scale\n",
    "ss = minmaxscaler(F.normalize(ss, p=2, dim=-1))\n",
    "sf = minmaxscaler(F.normalize(sf, p=2, dim=-1))\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    fs[i] += args.lamba * pred_t[i] *ss[i] + (1-args.lamba)*(1-pred_t[i]) * sf[i]\n",
    "auc.append(eval_roc_auc(data.y, fs))\n",
    "rec.append(eval_recall_at_k(data.y, fs, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb97a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9495\tRecall: 0.7800\n"
     ]
    }
   ],
   "source": [
    "# Weibo dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('str_pretrain.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('feat_pretrain.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('weibo')\n",
    "k_all = sum(data.y)\n",
    "# load sampled subgraph with 30 nodes\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/weibo/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 500\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:3' # distributed training and inference\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_weibo.pth'))\n",
    "classifier_model.eval()\n",
    "\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    \n",
    "    s_recover = model1.forward_di(data_pre)\n",
    "    f_recover = model2.forward_di(data_pre)\n",
    "    \n",
    "    for t in range(0,3):      \n",
    "        out, prop = classifier_model(model1.toden(s_recover,f_recover))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        prop_s = prop[:, 0]\n",
    "        prop_f = prop[:, 1]\n",
    "        correct += pred.eq(data_pre.y).sum().item()\n",
    "        loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "        s_recover = model1.conditional_di(s_recover,prop_s)\n",
    "        f_recover = model2.conditional_di(f_recover,prop_f)\n",
    "\n",
    "    ss = model1.loss_func(data_pre , s_recover)\n",
    "    sf = model2.loss_func(data_pre.x, f_recover)\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "\n",
    "y_label = np.array(y_label_list)\n",
    "pred_t = np.array(pred_list)\n",
    "data.y = data.y.bool().int()\n",
    "fs = torch.zeros_like(ss)\n",
    "# 0-1 scale\n",
    "ss = minmaxscaler(F.normalize(ss, p=2, dim=-1))\n",
    "sf = minmaxscaler(F.normalize(sf, p=2, dim=-1))\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    fs[i] += args.lamba * pred_t[i] *ss[i] + (1-args.lamba)*(1-pred_t[i]) * sf[i]\n",
    "auc.append(eval_roc_auc(data.y, fs))\n",
    "rec.append(eval_recall_at_k(data.y, fs, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3def28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8788\tRecall: 0.5042\n"
     ]
    }
   ],
   "source": [
    "# OTC dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('str_pretrain.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('feat_pretrain.pth')\n",
    "\n",
    "# preprocess as reference [3]\n",
    "df = pd.read_csv('/export/data/lixujia/bond/data/soc-sign-bitcoinotc.csv', header=None,\n",
    "                 names=[\"source\", \"target\", \"r\", \"t\"])\n",
    "benign_label = df.groupby('target')['r'].mean().to_frame()\n",
    "benign_label[\"benign\"] = benign_label['r'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "benign_label.drop(axis=1, columns='r', inplace=True)\n",
    "df2 = df.join(benign_label, on=\"source\")\n",
    "df3 = df2.drop(index=df2[(df2['benign'] == 0)].index.tolist())\n",
    "graph_label = df3.groupby('target')['r'].mean().to_frame()\n",
    "graph_label[\"fraud\"] = graph_label['r'].apply(lambda x: 1 if x <= -0.5 else 0)\n",
    "graph_label.drop(axis=1, columns='r', inplace=True)\n",
    "df_final = df.join(graph_label, on=\"target\")\n",
    "center_nodes = set(df_final['target'].tolist())\n",
    "t_avg = df.groupby('target')['r'].mean()\n",
    "s_avg = df.groupby('source')['r'].mean()\n",
    "t_deg = df.groupby('target')['r'].count()\n",
    "s_deg = df.groupby('source')['r'].count()\n",
    "t_attr = pd.concat([t_avg, t_deg], axis=1, ignore_index=False)\n",
    "s_attr = pd.concat([s_avg, s_deg], axis=1, ignore_index=False)\n",
    "t_attr.columns = ['0', '1']\n",
    "s_attr.columns = ['0', '1']\n",
    "\n",
    "G = nx.from_pandas_edgelist(df_final, 'source', 'target', ['r', 't'], create_using=nx.DiGraph())\n",
    "for node in G.nodes:\n",
    "    G.nodes[node]['fraud'] = 0\n",
    "    G.nodes[node]['t_avg'] = 0\n",
    "    G.nodes[node]['s_avg'] = 0\n",
    "    G.nodes[node]['t_deg'] = 0\n",
    "    G.nodes[node]['s_deg'] = 0\n",
    "for index, row in graph_label.iterrows():\n",
    "    G.nodes[index]['fraud'] = row['fraud']\n",
    "for index, row in t_attr.iterrows():\n",
    "    G.nodes[index]['t_avg'] = row['0']\n",
    "    G.nodes[index]['t_deg'] = row['1']\n",
    "for index, row in s_attr.iterrows():\n",
    "    G.nodes[index]['s_avg'] = row['0']\n",
    "    G.nodes[index]['s_deg'] = row['1']\n",
    "\n",
    "temp = from_networkx(G)\n",
    "\n",
    "x = torch.stack((temp.t_avg, temp.s_avg, temp.t_deg, temp.s_deg), 1).to(torch.float32)\n",
    "edge_index = temp.edge_index\n",
    "y = temp.fraud\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "k_all = sum(data.y)\n",
    "\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "dataset = BitcoinOTC(root='./pygod/pretrain/data/OTC')\n",
    "args.num_classes = 2\n",
    "args.num_features = 4\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:4'\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_OTC.pth'))\n",
    "classifier_model.eval()\n",
    "\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    \n",
    "    s_recover = model1.forward_di(data_pre)\n",
    "    f_recover = model2.forward_di(data_pre)\n",
    "    \n",
    "    for t in range(0,3):      \n",
    "        out, prop = classifier_model(model1.toden(s_recover,f_recover))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        prop_s = prop[:, 0]\n",
    "        prop_f = prop[:, 1]\n",
    "        correct += pred.eq(data_pre.y).sum().item()\n",
    "        loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "        s_recover = model1.conditional_di(s_recover,prop_s)\n",
    "        f_recover = model2.conditional_di(f_recover,prop_f)\n",
    "\n",
    "    ss = model1.loss_func(data_pre , s_recover)\n",
    "    sf = model2.loss_func(data_pre.x, f_recover)\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "\n",
    "y_label = np.array(y_label_list)\n",
    "pred_t = np.array(pred_list)\n",
    "data.y = data.y.bool().int()\n",
    "fs = torch.zeros_like(ss)\n",
    "# 0-1 scale\n",
    "ss = minmaxscaler(F.normalize(ss, p=2, dim=-1))\n",
    "sf = minmaxscaler(F.normalize(sf, p=2, dim=-1))\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    fs[i] += args.lamba * pred_t[i] *ss[i] + (1-args.lamba)*(1-pred_t[i]) * sf[i]\n",
    "auc.append(eval_roc_auc(data.y, fs))\n",
    "rec.append(eval_recall_at_k(data.y, fs, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
