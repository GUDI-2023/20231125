{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8599f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import warnings\n",
    "from pygod.metric import *\n",
    "from pygod.utils import load_data\n",
    "import time\n",
    "from pygod.pretrain.networks import Net\n",
    "from pygod.pretrain import util\n",
    "from torch_geometric.data import DataLoader\n",
    "from pygod.pretrain.inj_cora_dataset import InjCoraDataset\n",
    "from pygod.pretrain.OTC_dataset import BitcoinOTC\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import (from_networkx)\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    Dataset,\n",
    "    download_url,\n",
    "    extract_gz,\n",
    ")\n",
    "def minmaxscaler(data):\n",
    "    min = torch.min(data)\n",
    "    max = torch.max(data)\n",
    "    return (data - min)/(max-min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32eb8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8631\tRecall: 0.5217\n"
     ]
    }
   ],
   "source": [
    "# Cora dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('domi_cora.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('gae_cora.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_cora')\n",
    "k_all = sum(data.y)\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_cora/eval_30')\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_flickr/eval_10')\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_amazon/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 1433# 767 #500 #1433#767\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:0'\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_cora20.pth'))\n",
    "# classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_ama.pth'))\n",
    "classifier_model.eval()\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    out, prop = classifier_model(data_pre)\n",
    "    pred = out.max(dim=1)[1]\n",
    "\n",
    "    prop = prop[:, 1]\n",
    "    correct += pred.eq(data_pre.ys).sum().item()\n",
    "    loss += F.nll_loss(out, data_pre.ys, reduction='sum').item()\n",
    "\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "import numpy as np\n",
    "y_label = np.array(y_label_list)\n",
    "pred_label = np.array(pred_list)\n",
    "\n",
    "data.y = data.y.bool().int()\n",
    "\n",
    "\n",
    "score = model1.decision_score_\n",
    "score2 = model2.decision_score_\n",
    "score = minmaxscaler(score) # AUC: 0.9079±0.0000 (0.9079) AP: 0.9553±0.0000 (0.9553)      Recall: 0.8415±0.0000 (0.8415)\n",
    "score[score >= 0.65] = 0.65 # 50% 0.5 AUC: 0.8821±0.0000 (0.8821) AP: 0.9054±0.0000 (0.9054)      Recall: 0.8417±0.0000 (0.8417)\n",
    "\n",
    "\n",
    "score = F.normalize(score, p=2, dim=-1)\n",
    "score2 = F.normalize(score2, p=2, dim=-1)\n",
    "\n",
    "score1 = minmaxscaler(score)\n",
    "score2 = minmaxscaler(score2)\n",
    "\n",
    "\n",
    "# print(score1.mean(),score1.std(),score2.mean(),score2.std())\n",
    "emsemble =torch.zeros_like(score1)\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    if pred_list[i] == 1:\n",
    "        emsemble[i] = score1[i]\n",
    "    else:\n",
    "        emsemble[i] = score2[i]\n",
    "\n",
    "\n",
    "auc.append(eval_roc_auc(data.y, emsemble))\n",
    "rec.append(eval_recall_at_k(data.y, emsemble, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e9d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9231\tRecall: 0.8012\n"
     ]
    }
   ],
   "source": [
    "# Amazon dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('domi_ama.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('gae_ama.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_amazon')\n",
    "k_all = sum(data.y)\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_cora/eval_30')\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_flickr/eval_10')\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_amazon/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 767 # 767 #500 #1433#767\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:0'\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "# classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_cora20.pth'))\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_ama.pth'))\n",
    "classifier_model.eval()\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    out, prop = classifier_model(data_pre)\n",
    "    pred = out.max(dim=1)[1]\n",
    "\n",
    "    prop = prop[:, 1]\n",
    "    correct += pred.eq(data_pre.ys).sum().item()\n",
    "    loss += F.nll_loss(out, data_pre.ys, reduction='sum').item()\n",
    "\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "import numpy as np\n",
    "y_label = np.array(y_label_list)\n",
    "pred_label = np.array(pred_list)\n",
    "\n",
    "data.y = data.y.bool().int()\n",
    "\n",
    "\n",
    "score = model1.decision_score_\n",
    "score2 = model2.decision_score_\n",
    "\n",
    "\n",
    "score = F.normalize(score, p=2, dim=-1)\n",
    "score2 = F.normalize(score2, p=2, dim=-1)\n",
    "\n",
    "score1 = minmaxscaler(score)\n",
    "score2 = minmaxscaler(score2)\n",
    "\n",
    "\n",
    "# print(score1.mean(),score1.std(),score2.mean(),score2.std())\n",
    "emsemble =torch.zeros_like(score1)\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    if pred_list[i] == 1:\n",
    "        emsemble[i] = score1[i]\n",
    "    else:\n",
    "        emsemble[i] = score2[i]\n",
    "\n",
    "\n",
    "auc.append(eval_roc_auc(data.y, emsemble))\n",
    "rec.append(eval_recall_at_k(data.y, emsemble, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb170a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9349\tRecall: 0.5931\n"
     ]
    }
   ],
   "source": [
    "# Flickr dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('domi_fli.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('gae_fli.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('inj_flickr')\n",
    "k_all = sum(data.y)\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_cora/eval_30')\n",
    "dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_flickr/eval_10')\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_amazon/eval_30')\n",
    "\n",
    "# Inference with guided diffusion\n",
    "\n",
    "# load subgraph classifier and related parameters\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_classes = 2\n",
    "args.num_features = 500 # 767 #500 #1433#767\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:0'\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "classifier_model = Net(args).to(args.device)\n",
    "# classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_cora20.pth'))\n",
    "classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_flickr1.pth'))\n",
    "classifier_model.eval()\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "# calculate the prediction by subgraph classifier\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    out, prop = classifier_model(data_pre)\n",
    "    pred = out.max(dim=1)[1]\n",
    "\n",
    "    prop = prop[:, 1]\n",
    "    correct += pred.eq(data_pre.ys).sum().item()\n",
    "    loss += F.nll_loss(out, data_pre.ys, reduction='sum').item()\n",
    "\n",
    "    anomaly_label_graph.append(data_pre.label)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "import numpy as np\n",
    "y_label = np.array(y_label_list)\n",
    "pred_label = np.array(pred_list)\n",
    "\n",
    "data.y = data.y.bool().int()\n",
    "\n",
    "score = model1.decision_score_\n",
    "score2 = model2.decision_score_\n",
    "\n",
    "\n",
    "score = F.normalize(score, p=2, dim=-1)\n",
    "score2 = F.normalize(score2, p=2, dim=-1)\n",
    "\n",
    "score1 = minmaxscaler(score)\n",
    "score2 = minmaxscaler(score2)\n",
    "\n",
    "\n",
    "# print(score1.mean(),score1.std(),score2.mean(),score2.std())\n",
    "emsemble =torch.zeros_like(score1)\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    if pred_list[i] == 1:\n",
    "        emsemble[i] = score1[i]\n",
    "    else:\n",
    "        emsemble[i] = score2[i]\n",
    "\n",
    "\n",
    "auc.append(eval_roc_auc(data.y, emsemble))\n",
    "rec.append(eval_recall_at_k(data.y, emsemble, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb97a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9495\tRecall: 0.7800\n"
     ]
    }
   ],
   "source": [
    "# Weibo dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "# load pretrained model\n",
    "# pretrain structure reconstruction networks\n",
    "model1 = torch.load('gae_weibo.pth')\n",
    "# pretrain feature reconstruction networks\n",
    "model2 = torch.load('gae_weibo.pth')\n",
    "\n",
    "# load specific dataset\n",
    "data = load_data('weibo')\n",
    "k_all = sum(data.y)\n",
    "# # dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_cora/eval_30')\n",
    "# dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_flickr/eval_10')\n",
    "# # dataset = InjCoraDataset(root='./pygod/pretrain/data/inj_amazon/eval_30')\n",
    "\n",
    "# # Inference with guided diffusion\n",
    "\n",
    "# # load subgraph classifier and related parameters\n",
    "# parser = util.parser\n",
    "# args = parser.parse_args(args=[])\n",
    "# args.num_classes = 2\n",
    "# args.num_features = 500 # 767 #500 #1433#767\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "#     args.device = 'cuda:0'\n",
    "# else:\n",
    "#     args.device = 'cpu'\n",
    "# loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# classifier_model = Net(args).to(args.device)\n",
    "# # classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_cora20.pth'))\n",
    "# classifier_model.load_state_dict(torch.load('./pygod/pretrain/latest_flickr1.pth'))\n",
    "# classifier_model.eval()\n",
    "# correct = 0.\n",
    "# loss = 0.\n",
    "# anomaly_label_graph = []\n",
    "# anomaly_pred_graph = []\n",
    "\n",
    "# # calculate the prediction by subgraph classifier\n",
    "# for data_pre in loader:\n",
    "#     data_pre = data_pre.to(args.device)\n",
    "#     out, prop = classifier_model(data_pre)\n",
    "#     pred = out.max(dim=1)[1]\n",
    "\n",
    "#     prop = prop[:, 1]\n",
    "#     correct += pred.eq(data_pre.ys).sum().item()\n",
    "#     loss += F.nll_loss(out, data_pre.ys, reduction='sum').item()\n",
    "\n",
    "#     anomaly_label_graph.append(data_pre.label)\n",
    "#     anomaly_pred_graph.append(pred)\n",
    "\n",
    "# y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "# pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "# import numpy as np\n",
    "# y_label = np.array(y_label_list)\n",
    "# pred_label = np.array(pred_list)\n",
    "\n",
    "data.y = data.y.bool().int()\n",
    "\n",
    "score = model1.decision_score_\n",
    "score2 = model2.decision_score_\n",
    "\n",
    "\n",
    "score = F.normalize(score, p=2, dim=-1)\n",
    "score2 = F.normalize(score2, p=2, dim=-1)\n",
    "\n",
    "score1 = minmaxscaler(score)\n",
    "score2 = minmaxscaler(score2)\n",
    "\n",
    "\n",
    "# print(score1.mean(),score1.std(),score2.mean(),score2.std())\n",
    "emsemble =torch.zeros_like(score1)\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    if True:\n",
    "        emsemble[i] = score1[i]\n",
    "    else:\n",
    "        emsemble[i] = score2[i]\n",
    "\n",
    "\n",
    "auc.append(eval_roc_auc(data.y, emsemble))\n",
    "rec.append(eval_recall_at_k(data.y, emsemble, k_all))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3def28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/export/data/lixujia/anaconda3/envs/pygod/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8788\tRecall: 0.5042\n"
     ]
    }
   ],
   "source": [
    "# OTC dataset\n",
    "\n",
    "auc, rec = [], []\n",
    "df = pd.read_csv('/export/data/lixujia/bond/data/soc-sign-bitcoinotc.csv', header=None,\n",
    "                 names=[\"source\", \"target\", \"r\", \"t\"])\n",
    "benign_label = df.groupby('target')['r'].mean().to_frame()\n",
    "benign_label[\"benign\"] = benign_label['r'].apply(lambda x: 1 if x >= 0.5 else 0)  # 确认是好用户\n",
    "# 合并回原df\n",
    "benign_label.drop(axis=1, columns='r', inplace=True)\n",
    "df2 = df.join(benign_label, on=\"source\")\n",
    "df3 = df2.drop(index=df2[(df2['benign'] == 0)].index.tolist())  # 只留下benign的source\n",
    "# 找到异常点\n",
    "graph_label = df3.groupby('target')['r'].mean().to_frame()\n",
    "graph_label[\"fraud\"] = graph_label['r'].apply(lambda x: 1 if x <= -0.5 else 0)\n",
    "graph_label.drop(axis=1, columns='r', inplace=True)\n",
    "df_final = df.join(graph_label, on=\"target\")\n",
    "center_nodes = set(df_final['target'].tolist())\n",
    "\n",
    "# 添加节点特征\n",
    "# 入均分\n",
    "t_avg = df.groupby('target')['r'].mean()\n",
    "# 出均分\n",
    "s_avg = df.groupby('source')['r'].mean()\n",
    "# 入度\n",
    "t_deg = df.groupby('target')['r'].count()\n",
    "# 出度\n",
    "s_deg = df.groupby('source')['r'].count()\n",
    "t_attr = pd.concat([t_avg, t_deg], axis=1, ignore_index=False)\n",
    "s_attr = pd.concat([s_avg, s_deg], axis=1, ignore_index=False)\n",
    "t_attr.columns = ['0', '1']\n",
    "s_attr.columns = ['0', '1']\n",
    "\n",
    "# 建立完整的图\n",
    "G = nx.from_pandas_edgelist(df_final, 'source', 'target', ['r', 't'], create_using=nx.DiGraph())\n",
    "for node in G.nodes:\n",
    "    G.nodes[node]['fraud'] = 0\n",
    "    G.nodes[node]['t_avg'] = 0\n",
    "    G.nodes[node]['s_avg'] = 0\n",
    "    G.nodes[node]['t_deg'] = 0\n",
    "    G.nodes[node]['s_deg'] = 0\n",
    "for index, row in graph_label.iterrows():\n",
    "    G.nodes[index]['fraud'] = row['fraud']\n",
    "for index, row in t_attr.iterrows():\n",
    "    G.nodes[index]['t_avg'] = row['0']\n",
    "    G.nodes[index]['t_deg'] = row['1']\n",
    "for index, row in s_attr.iterrows():\n",
    "    G.nodes[index]['s_avg'] = row['0']\n",
    "    G.nodes[index]['s_deg'] = row['1']\n",
    "\n",
    "temp = from_networkx(G)\n",
    "\n",
    "x = torch.stack((temp.t_avg, temp.s_avg, temp.t_deg, temp.s_deg), 1).to(torch.float32)  # 节点属性\n",
    "edge_index = temp.edge_index\n",
    "# y = torch.stack((temp.fraud), 1).to(torch.float32)  # 节点属性\n",
    "y = temp.fraud\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "\n",
    "\n",
    "model = torch.load('domi_OTC.pth')\n",
    "score = model.decision_score_\n",
    "\n",
    "k_all = sum(data.y)\n",
    "\n",
    "model2 = torch.load('gae_OTC.pth')\n",
    "score2 = model2.decision_score_\n",
    "\n",
    "\n",
    "\n",
    "score = minmaxscaler(score) \n",
    "\n",
    "score = F.normalize(score, p=2, dim=-1)\n",
    "score2 = F.normalize(score2, p=2, dim=-1)\n",
    "\n",
    "score1 = minmaxscaler(score)\n",
    "score2 = minmaxscaler(score2)\n",
    "\n",
    "emsemble =torch.zeros_like(score1)\n",
    "\n",
    "\n",
    "#\n",
    "# 调取pretrain model\n",
    "parser = util.parser\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "dataset = BitcoinOTC(root='./pygod/pretrain/data/OTC')\n",
    "args.num_classes = 2\n",
    "args.num_features = 4#767\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.device = 'cuda:0'\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "pretrain_model = Net(args).to(args.device)\n",
    "pretrain_model.load_state_dict(torch.load('./pygod/pretrain/latest_OTC20.pth'))\n",
    "pretrain_model.eval()\n",
    "correct = 0.\n",
    "loss = 0.\n",
    "anomaly_label_graph = []\n",
    "anomaly_pred_graph = []\n",
    "\n",
    "\n",
    "for data_pre in loader:\n",
    "    data_pre = data_pre.to(args.device)\n",
    "    out, prop = pretrain_model(data_pre)\n",
    "    pred = out.max(dim=1)[1]\n",
    "\n",
    "    prop = prop[:, 1]\n",
    "    correct += pred.eq(data_pre.y).sum().item()\n",
    "    loss += F.nll_loss(out, data_pre.y, reduction='sum').item()\n",
    "\n",
    "    anomaly_label_graph.append(data_pre.y)\n",
    "    anomaly_pred_graph.append(pred)\n",
    "\n",
    "y_label_list = torch.cat(anomaly_label_graph, 0).view(-1).tolist()\n",
    "pred_list = torch.cat(anomaly_pred_graph, 0).view(-1).tolist()\n",
    "# y_label = np.array(y_label_list)\n",
    "# pred_label = np.array(pred_list)\n",
    "\n",
    "data.y = data.y.bool().int()\n",
    "\n",
    "for i, y in enumerate(data.y):\n",
    "    # print (y, y_label_list[i])\n",
    "    assert y == y_label_list[i]\n",
    "    if pred_list[i] == 1:\n",
    "    # if ys[i] == 1:\n",
    "        emsemble[i] = score1[i]\n",
    "    else:\n",
    "        emsemble[i] = score2[i]\n",
    "\n",
    "\n",
    "auc.append(eval_roc_auc(data.y, emsemble))\n",
    "rec.append(eval_recall_at_k(data.y, emsemble, k_all))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(\"AUC: {:.4f}\\t\"\n",
    "      \"Recall: {:.4f}\"\n",
    "      .format(np.mean(auc),\n",
    "              np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d253895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
